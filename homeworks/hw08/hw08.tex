\documentclass[12pt]{article}

\include{preamble}
\newcommand{\compexpl}{Compute explicitly as a number rounded to two decimals.}

\newtoggle{professormode}
\toggletrue{professormode} %STUDENTS: DELETE or COMMENT this line



\title{MATH 341 / 650 Spring 2017 Homework \#8}

\author{Professor Adam Kapelner} %STUDENTS: write your name here

\iftoggle{professormode}{
\date{Due \textit{in class}, Tuesday, May 16, 2017 \\ \vspace{0.5cm} \small (this document last updated \today ~at \currenttime)}
}

\renewcommand{\abstractname}{Instructions and Philosophy}

\begin{document}
\maketitle

\iftoggle{professormode}{
\begin{abstract}
The path to success in this class is to do many problems. Unlike other courses, exclusively doing reading(s) will not help. Coming to lecture is akin to watching workout videos; thinking about and solving problems on your own is the actual ``working out.''  Feel free to \qu{work out} with others; \textbf{I want you to work on this in groups.}

Reading is still \textit{required}. For this homework set review conjugate mixture priors, sampling algorithms, the Newton Raphson algorithm, the E-M algorithm and Gibbs sampling.

The problems below are color coded: \ingreen{green} problems are considered \textit{easy} and marked \qu{[easy]}; \inorange{yellow} problems are considered \textit{intermediate} and marked \qu{[harder]}, \inred{red} problems are considered \textit{difficult} and marked \qu{[difficult]} and \inpurple{purple} problems are extra credit. The \textit{easy} problems are intended to be ``giveaways'' if you went to class. Do as much as you can of the others; I expect you to at least attempt the \textit{difficult} problems.  Problems marked \qu{[MA]} are for the masters students only (those enrolled in the 650 course). For those in 341, doing these questions will count as extra credit.

This homework is worth 100 points but the point distribution will not be determined until after the due date. See syllabus for the policy on late homework.

Up to 10 points are given as a bonus if the homework is typed using \LaTeX. Links to instaling \LaTeX~and program for compiling \LaTeX~is found on the syllabus. You are encouraged to use \url{overleaf.com}. If you are handing in homework this way, read the comments in the code; there are two lines to comment out and you should replace my name with yours and write your section. The easiest way to use overleaf is to copy the raw text from hwxx.tex and preamble.tex into two new overleaf tex files with the same name. If you are asked to make drawings, you can take a picture of your handwritten drawing and insert them as figures or leave space using the \qu{$\backslash$vspace} command and draw them in after printing or attach them stapled.

The document is available with spaces for you to write your answers. If not using \LaTeX, print this document and write in your answers. I do not accept homeworks which are \textit{not} on this printout. Keep this first page printed for your records.

\end{abstract}

\thispagestyle{empty}
\vspace{1cm}
NAME: \line(1,0){380}
\clearpage
}


\problem{These are questions about McGrayne's book, chapters 15 and 16.}

\begin{enumerate}

\easysubproblem{During the H-Bomb search in Spain and its coastal regions, RAdm. William Guest was busy sending ships here, there and everywhere even if the ships couldn't see the bottom of the ocean. How did Richardson use those useless searches?}\spc{2}

\intermediatesubproblem{When the Navy was looking for the \textit{Scorpion} submarine, they used Monte Carlo methods (which we will see in class soon). How does the description of these methods by Richardson (p199) remind you of the \qu{sampling} techniques to approximate integrals we did in class?}\spc{4}

\intermediatesubproblem{What is a Kalman filter? Read about it online and write a few descriptive sentences.}\spc{4}


\intermediatesubproblem{Where do frequentist methods practically break down? (end of chapter 15)}\spc{4}

\easysubproblem{What was the main problem facing Bayesian Statistics in the early 1980's?}\spc{4}

\intermediatesubproblem{What is the \qu{curse of dimensionality?}}\spc{4}

\easysubproblem{How did Bayesian Statistics help sociologists?}\spc{4}

\easysubproblem{How did Gibbs sampling come to be?}\spc{3}

\easysubproblem{Were the Geman brothers the first to discover the Gibbs sampler?}\spc{4}

\easysubproblem{Who officially discovered the expectation-maximization (EM) algorithm? And who \textit{really} discovered it?}\spc{4}

\intermediatesubproblem{How did Bayesians \qu{break} the curse of dimensionality?}\spc{4}

\intermediatesubproblem{Consider the integrals we use in class to find expectations or to approximate PDF's / PMF's --- how can they be replaced?}\spc{4}

\easysubproblem{What did physicists call \qu{Markov Chain Monte Carlo} (MCMC)? (p222)}\spc{1}

\easysubproblem{Why is sampling called \qu{Monte Carlo} and who named it that?}\spc{4}

\easysubproblem{The Metropolis-Hastings (MH) Algorithm is world famous and used in myriad applications. Why didn't Hastings get any credit?}\spc{4}

\easysubproblem{The combination of Bayesian Statistics + MCMC has been called ... (p224)}\spc{1}


\extracreditsubproblem{p225 talks about Thomas Kuhn's ideas of \qu{paradigm shifts.} What is a \qu{paradigm shift} and does Bayesian Statistics + MCMC qualify?}\spc{8}

\easysubproblem{How did the \href{http://www.mrc-bsu.cam.ac.uk/software/bugs/}{BUGS} software change the world?}\spc{4}

\easysubproblem{Lindley said that Bayesian Statistics would win out over Frequentist Statistics because it was more logical. What in reality was the reason for the eventual victory of Bayes?}\spc{4}

\extracreditsubproblem{One of my PhD advisors, \href{https://statistics.wharton.upenn.edu/profile/563/}{Ed George} at Wharton told me that \qu{Bayesian Statistics is really `knowledge engineering.'} Is this true? Explain.}\spc{15}

%\extracreditsubproblem{Take a look at the software \href{http://mc-stan.org/}{Stan}. What kind of potential does it have to change the world? Note: I had an opportunity to work on Stan as a postdoc (right after I finished his PhD) but chose to come to QC instead.}\spc{10}

\end{enumerate}


\problem{These are questions about the prior being a mixture distribution.}

\begin{enumerate}
\easysubproblem{Let's say you have a prior distribution $\prob{\theta}$ which is a mixture of $M$ distributions that you are mixing. Call them, $\probsub{1}{\theta}, \probsub{2}{\theta}, \ldots, \probsub{M}{\theta}$ and the mixing proportions are $\rho_1, \rho_2, \ldots, \rho_M$. Write the prior below using the formula we discussed in class.}\spc{4}

\easysubproblem{Is there a restriction on the mixing proportions, $\rho_1, \rho_2, \ldots, \rho_M$? Discuss.}\spc{4}

%\intermediatesubproblem{Why is a mixture distribution sometimes called a \qu{convex combination?} What's \qu{convex} about it? }\spc{4}

\easysubproblem{Explain how you can use a mixture distribution of betas to approximate any continuous distribution (within reason) with support $\zeroonecl$.}\spc{4}

\intermediatesubproblem{Rederive from the class notes that the posterior is also a mixture distribution. What distributions does it mix? What are the new mixing proportions ($\rho_1', \rho_2', \ldots, \rho_M'$)?}\spc{8}


\intermediatesubproblem{Rederive the form of $\thetahat_{MMSE}$.}\spc{8}


\hardsubproblem{Derive the posterior predictive distribution.}\spc{13}

\end{enumerate}

\input{R_functions_table}

\problem{If the prior is a mixture distribution of conjugate priors, then the math is convenient. We explore a case like this here.}

\begin{enumerate}
\easysubproblem{Let's say you have a sample of heights from people in the U.S. but you don't know if the people are males or females. According to \href{https://en.wikipedia.org/wiki/Human_height}{wikipedia}, the average male in America is 5' 9.5" or 69.5" and the average female is 5' 4'' or 64". Let's design a mixture prior. Assume equal mixing. What is $\mu_{0,M}$ for the males and what is $\mu_{0,F}$ for females? Use inches as the unit to keep things simple.}\spc{8}

\easysubproblem{If there is no reason to suspect that the sample you have is male or female, what are the mixing proportions, $\rho_M$ and $\rho_F$?}\spc{4}

\easysubproblem{Use $\theta$ for the mean height in your sample. Assume the standard deviation is $\sigma = 2.8''$. What is the likelihood model? Assume a normal likelihood model (since the sum of normals will be normal... you'll see that happen later).}\spc{8}

\intermediatesubproblem{According to Wikipedia, there were 895 males and 980 females? Using that information and your answer to (a) and (b) and the information given in (c), construct $\prob{\theta}$. Keep $\rho_M$ and $\rho_F$ as you had in (b). This is review for the final as a question similar to this one was on the midterm.}

Hint: $\tausq_M = \sigsq / 895$ and $\tausq_F = \sigsq / 980$.

~\spc{5}


\hardsubproblem{Derive the posterior distribution, $\cprob{\theta}{X, \sigsq = 2.8"^2}$.}

I will help you. We first need to compute $\rho_M' = \rho_M \frac{\probsub{M}{X}}{\probsub{M}{X} + \probsub{F}{X}}$ where:

\beqn
\probsub{M}{X} &:=& \probsub{M}{X_1, \ldots, X_n} \\
&=&  \int_\reals \cprob{X_1, \ldots, X_n}{\theta} \probsub{M}{\theta} d\theta \\
&=&  \int_\reals \parens{\prod_{i=1}^n \normnot{\theta}{\sigsq}} \normnot{\mu_M}{\tausq_M} d\theta \\
&=&  \int_\reals \parens{\prod_{i=1}^n \oneoversqrt{2\pi \sigsq} \exp{-\oneover{2\sigsq} \squared{x_i - \theta}}} \oneoversqrt{2\pi \tausq_M} \exp{-\oneover{2\tausq_M} \squared{\theta - \mu_M}}d\theta \\
&=&  \tothepow{\oneoversqrt{2\pi \sigsq}}{n} \int_\reals  \exp{-\oneover{2\sigsq} {\sum_{i=1}^n \squared{x_i - \theta}}}  \normnot{\mu_M}{\tausq_M}d\theta \\
&=&  \tothepow{\oneoversqrt{2\pi \sigsq}}{n} \int_\reals  \exp{-\oneover{2\sigsq} \parens{(n-1)s^2 + n(\xbar - \theta)^2}}  \normnot{\mu_M}{\tausq_M}d\theta \\
&=&  \tothepow{\oneoversqrt{2\pi \sigsq}}{n} e^{-\oneover{2\sigsq} \parens{(n-1)s^2}} \int_\reals  \exp{-\oneover{2\sigsq / n} (\xbar - \theta)^2}  \normnot{\mu_M}{\tausq_M}d\theta \\
&=&  \tothepow{\oneoversqrt{2\pi \sigsq}}{n} e^{-\oneover{2\sigsq} \parens{(n-1)s^2}} \sqrt{2\pi\sigsq/n} \times \\
&& \int_\reals  \oneoversqrt{2\pi\sigsq/n} \exp{-\oneover{2\sigsq / n} (\xbar - \theta)^2}  \normnot{\mu_M}{\tausq_M}d\theta \\
&=&  \tothepow{\oneoversqrt{2\pi \sigsq}}{n} e^{-\oneover{2\sigsq} \parens{(n-1)s^2}} \sqrt{2\pi\sigsq/n} {\int_\reals  \normnot{\xbar}{\sigsq/n}  \normnot{\mu_M}{\tausq_M}d\theta} \\
&=&  \tothepow{\oneoversqrt{2\pi \sigsq}}{n} e^{-\oneover{2\sigsq} \parens{(n-1)s^2}} \sqrt{2\pi\sigsq/n}   ~\normnot{\mu_M}{\sigsq/n + \tausq_M} \\
\eeqn

Note how everything before the normal density is a constant which cancels out when we compute the posterior weight:

\beqn
\rho_M' &=& \rho_M \frac{\probsub{M}{X}}{\probsub{M}{X} + \probsub{F}{X}} \\
&=& \half \frac{\normnot{\mu_M}{\sigsq/n + \tausq_M}}{\normnot{\mu_M}{\sigsq/n + \tausq_M} + \normnot{\mu_F}{\sigsq/n + \tausq_F}} \\
&=& \half \frac{\oneoversqrt{2\pi \parens{\sigsq/n + \tausq_M}} e^{-\oneover{2 \parens{\sigsq/n + \tausq_M}} \squared{\xbar - \mu_M}} }{
\oneoversqrt{2\pi \parens{\sigsq/n + \tausq_M}} e^{-\oneover{2 \parens{\sigsq/n + \tausq_M}} \squared{\xbar - \mu_M}} 
+
\oneoversqrt{2\pi \parens{\sigsq/n + \tausq_F}} e^{-\oneover{2 \parens{\sigsq/n + \tausq_F}} \squared{\xbar - \mu_F}} 
}
\eeqn

and $\rho_F'$ can be calculated similarly. Your answer for (f) should show that $\rho_F' \approx 100\%$ and $\rho_M' \approx 0\%$.
~\spc{13}

\easysubproblem{You get a sample of $n=10$ where the heights are: \\

$X = \braces{\texttt{62.8, 60.2, 58.0, 62.7, 66.4, 58.6, 64.0, 59.8, 66.7, 62.0}}$.\\

Given your answer to (e), what is the posterior distribution $\cprob{\theta}{X, ~\sigsq = 2.8"^2}$ now?
}\spc{8}

\easysubproblem{What is the posterior mixture proportion $\rho_M'$? Does this make sense given the data?}\spc{9}

\hardsubproblem{[M.A.] Your answer to (g) was a \qu{best guess} but it was not Bayesian. Let's put a prior on $\rho \sim \betanot{\alpha}{\beta}$. This is now known as a \qu{hierarchical Bayesian model} since there's both a prior on the parameters and a prior on the prior called a \qu{hyperprior} with \qu{hyperhyperparameters.} Write out the posterior as best as you could below.}\spc{7}

\extracreditsubproblem{Disregard (h). For general $\Xoneton$ and $\rho$ and known $\sigsq$, find the form of $\cprob{X^*}{\Xoneton, \sigsq}$. Do on a separate page.}\spc{15}

\end{enumerate}


\problem{We practice using the Newton-Raphson algorithm here using the beta distribution.}

\begin{enumerate}

\easysubproblem{If $\theta \sim \betanot{10}{30}$ then what is the prior expectation and the prior mode? Use the formulas we gave in class before the first midterm.}\spc{4}

\easysubproblem{State the Newton-Raphson algorithm below.}\spc{3}

\hardsubproblem{We will make sure this formula for the mode of a beta distribution is correct by using Newton-Raphson. Begin at the expectation and iterate twice. By what percentage are you off by?}\spc{11}


\easysubproblem{[M.A.] State the generalized Newton-Raphson algorithm (for arbitrary dimension) below. Wikipedia is your friend.}\spc{6}


\easysubproblem{[M.A.] In lecture 20 we discussed the likelihood mixture model 

\beqn
\Xoneton~|~\thetavec \iid \rho ~\normnot{\theta_1}{\sigsq_1} + (1-\rho)~ \normnot{\theta_2}{\sigsq_2} ~\text{where $\thetavec := \bracks{\theta_1,~\sigsq_1,~\theta_2,~\sigsq_2,~\rho}$}.
\eeqn

Explain why it would be a disaster to find $\thetavechatmle$ via the generalized Newton-Raphson algorithm.}\spc{4}

\end{enumerate}


\problem{We practice using the E-M algorithm here using an example similar to the lecture.}

\begin{enumerate}

\easysubproblem{What does \qu{data augmentation} mean? Write also the integral expression that explains data augmentation.}\spc{7}

\easysubproblem{Imagine we have the likelihood model found in 4(e). If we knew the membership of each $X_i$ denoted by $I_i$ where if $I_i=1$ it means the $i$th observation belongs to the first distribution i.e. $\normnot{\theta_1}{\sigsq_1}$ and if $I_i=0$ it means it did not belong to the first distribution, it belongs to the second distribution i.e. $\normnot{\theta_2}{\sigsq_2}$. What is the likelihood now?}\spc{5}

\easysubproblem{Consider the situation where we are once again trying to estimate mean heights. Here, we are trying to estimate the mean height of males which is denoted $\theta_1$, the mean height of females denoted $\theta_2$ and the variances denoted by $\sigsq_1$ and $\sigsq_2$. If $n=10$ and you know that the first sample of $n_1 = 5$ heights comes from males: $71.6, 67.0, 70.0, 66.5, 72.2$ (in inches) and the second sample of heights $n_2 = 5$ comes from females: $63.1, 64.3, 59.2, 60.0, 68.3$ (in inches), what are your best estimates of $\theta_1$, $\theta_2$, $\sigsq_1$, $\sigsq_2$ and $\rho$? This is marked easy for a reason.}\spc{5}

\intermediatesubproblem{An 11th data point comes in and its height is $x_{11} = 68.1"$ but you do not know if it's from a male or a female. Could this new data point help with estimating $\theta_1$ even though you do not know it's a measurement from a male?}\spc{4}

\easysubproblem{What is the E-M algorthim generally speaking? And is it Bayesian in its general form?}\spc{8}


\hardsubproblem{Implement the E-M algorithm here by starting with the values from (c) and that $\prob{I_{11}} = \half$ initially, do two iterations. What are your new values for $\theta_1$ and $\theta_2$?}\spc{17}

\hardsubproblem{What is the probability the 11th measurement comes from a male?}\spc{20}

\extracreditsubproblem{Write up a rudimentary proof for the convergence of the E-M algorithm. Do on a separate page.}\spc{15}
\end{enumerate}


\end{document}


\problem{These are questions about McGrayne's book, chapters 11--14.}

\begin{enumerate}

\easysubproblem{Did Savage like Shlaifer? Yes / No and why?}\spc{3}

\easysubproblem{How did Neyman-Pearson approach statistical decision theory? What is the weakness to this approach? (p145)}\spc{3}

\easysubproblem{Who popularized \qu{probability trees} (and \qu{tree flipping}) similar to exercises we did in Math 241?}\spc{1}

\easysubproblem{Where are Bayesian methods taught more widely than any other discipline in academia?}\spc{2}

\easysubproblem{Despite the popularity of his Bayesian textbook on business decision theory, why didn't Schlaifer's Bayesianism catch on in the real world of business executives making decisions?}\spc{3}

\easysubproblem{Why did the pollsters fail (big time) to predict Harry Truman's victory in the 1948 presidential election?}\spc{2}

\easysubproblem{When does the diference between Bayesianism and Frequentism grow \qu{immense}?}\spc{3}

\easysubproblem{How did Mosteller demonstrate that Madison wrote the 12 Federalist papers of unknown authorship?}\spc{3}

\easysubproblem{Write a one paragraph biography of John Tukey.}\spc{6}

\easysubproblem{Why did Alfred Kinsey's wife want to poison John Tukey?}\spc{2}

\easysubproblem{Tukey helped NBC with polling predictions for the presidential campaign. What was NBC's polling algorithm based on?}\spc{2}

\easysubproblem{Why is \qu{objectivity an heirloom ... and ... a fallacy?}}\spc{2}

\easysubproblem{Why do you think Tukey called Bayes Rule by the name \qu{borrowing strength?}}\spc{2}

\easysubproblem{Why is it that we don't know a lot of Bayes Rule's modern history?}\spc{2}

\easysubproblem{Generally speaking, how does Nate Silver predict elections?}\spc{2}

\easysubproblem{How many Bayesians of import were there in 1979?}\spc{1}

\easysubproblem{What advice did Chernoff give to Susan Holmes? (Note: Susan Holmes was my undergraduate advisor).}\spc{3}

\easysubproblem{How did Rasmussen's team estimate the probability of a nuclear plant core meltdown?}\spc{4}

\easysubproblem{How did the Three Mile Island accident vindicate Rasmussen's committee report?}\spc{4.5}


\end{enumerate}

\input{R_functions_table}


\problem{We will review classical \textit{frequentist} concepts from \qu{Math 241/242}. Much of this can be drawn from lecture 14 first page.}

\begin{enumerate}

\easysubproblem{If $\Xoneton \iid \normnot{\theta}{\sigsq}$ and $\Xbar := \oneover{n}\sum_{i=1}^n X_i$, what is the distribution of the following:
\beqn
\frac{\Xbar - \theta}{\frac{\sigma}{\sqrt{n}}} \sim ~~~~~~~~~~~~~~~~~
\eeqn}~\spc{1}

\easysubproblem{If $\Xoneton \iid \normnot{\theta}{\sigsq}$ and $\Xbar := \oneover{n}\sum_{i=1}^n X_i$, what is the distribution of $\Xbar$ assuming $\sigsq$ is known? This can be derived from (a) or found in your Math 241 notes.}\spc{1}

\easysubproblem{Write the definition of $S^2$, the r.v. which is the sample variance \textit{estimator}. Hint: use capital letters.}\spc{3}

\easysubproblem{Write the definition of $S$, the sample standard deviation \textit{estimator} (or standard error estimator --- both terms are synonymous). Hint: use capital letters.}\spc{3}

\easysubproblem{Write the definition of $s^2$, the r.v. which is the sample variance \textit{estimate}. Hint: use lowercase letters.}\spc{3}



\easysubproblem{This answer is in the notes. If $\Xoneton \iid \normnot{\theta}{\sigsq}$ and $\Xbar := \oneover{n}\sum_{i=1}^n X_i$, what is the distribution of the following where $S$ is defined as in (d):\\

\beqn
\frac{\Xbar - \theta}{\frac{S}{\sqrt{n}}} \sim
\eeqn}

\easysubproblem{Write the PDF of the general (also called non-standard) $T$ distribution below. You need to use the notation given in class.}\spc{4}

\easysubproblem{What is the kernel of the nonstandard $T$?}\spc{4}

\intermediatesubproblem{What is the distribution of $\Xbar$ assuming $\sigsq$ is \textit{unknown}? This will differ from (b).  Use the answer from part (k) above and the fact that $a T_\nu + c \sim T_\nu(c, a)$ which means that if you shift and scale a T with $\nu$ degrees of freedom, you get a nonstandard $T_\nu$ with the new center and scaling as parameters.}\spc{4}

\end{enumerate}


\problem{Now we will move to the Bayesian normal-normal model for estimating both the mean and variance and demonstrate similarities with the classical results.}

\begin{enumerate}

\easysubproblem{If $\Xoneton~|~\theta, \sigsq \iid \normnot{\theta}{\sigsq}$ and $X$ represents all $\Xoneton$, in HW6 6(c) you found the kernel of $\theta,~\sigsq~|~X = x$. Do so again below but this time use the substitution that we made in class:

\beqn
\sum_{i=1}^n (x_i - \theta)^2 = (n-1)s^2 + n(\xbar - \theta)^2
\eeqn

where $s^2$ is your answer from 2(e). We do this here because this substitution is important for what comes next.}\spc{4}

\intermediatesubproblem{If $\prob{\theta,~\sigsq} \propto \oneover{\sigsq}$, show that this is a conjugate prior for the posterior of both the mean and variance, $\cprob{\theta,~\sigsq}{X}$. We called this two-dimensional distribution the \qu{normal-inverse-gamma} distribution but we did not go into details about it.}\spc{8}

\intermediatesubproblem{Using Bayes Rule, break up $\cprob{\theta,~\sigsq}{X}$ into two pieces.}\spc{1}

\intermediatesubproblem{Using your answer from (c), explain how you can create samples $\bracks{\theta_s, \sigsq_s}$ from the distribution $\cprob{\theta,~\sigsq}{X}$.}\spc{7}

\hardsubproblem{Using these samples, how would you estimate $\cexpe{\theta}{X}$ and $\cexpe{\sigsq}{X}$? Why is $\cexpe{\theta}{X}$ of paramount importance?}\spc{5}

\hardsubproblem{[MA] Using these samples, how would you estimate $\corr{\theta~|~X}{~\sigsq~|~X}$ i.e. the correlation between the posterior distributions of the two parameters?}\spc{5}

%\easysubproblem{If $\Xoneton~|~\theta, \sigsq \iid \normnot{\theta}{\sigsq}$ and $\theta \sim \normnot{\mu_0}{\tausq}$ write the distribution of $\theta~|~X,\sigsq$. Hint: it's in the notes and it was HW6 6(d). Note this problem is independent of the other problems.}\spc{5}

\easysubproblem{Find $\cprob{\theta}{X,~\sigsq}$ by using the full posterior and then conditioning on $\sigsq$. You should get the same answer as we did before the midterm.}\spc{3}

\easysubproblem{Find $\cprob{\sigsq}{X,~\theta}$ by using the full posterior and then conditioning on $\theta$. You should get the same answer as we did before the midterm.}\spc{3}

\hardsubproblem{Show that $\cprob{\theta}{X}$ is a non-standard $T$ distribution (assume prior in b). The answer is in the notes, but try to do it yourself.}\spc{5}

\hardsubproblem{Show that $\cprob{\sigsq}{X}$ is an inverse gamma distribution and find its parameters.}\spc{7}

%\intermediatesubproblem{How does this compare to 2(j)? Note that $X \sim \invgammanot{\alpha}{\beta}$ then $cX \sim \invgammanot{\alpha}{\frac{\beta}{c}}$.}\spc{2}

\easysubproblem{Write down the distribution of $X^*~|~X$ which is in the notes (lec 14, page 6). Note that the answer I wrote down is for the non-informative prior only.}\spc{1}

\extracreditsubproblem{[MA] Prove (k).}\spc{20}


\easysubproblem{Explain how to sample from the distribution of $X^*~|~X$. Hint: write it as a double integrel of two conditional distributions and a marginal distribution (all conditional on $X$).}\spc{9}

\easysubproblem{Now consider the informative conjugate prior of

\beqn
\prob{\theta,~\sigsq} = \cprob{\theta}{\sigsq} \prob{\sigsq} =  \normnot{\mu_0}{\frac{\sigsq}{m}} \invgammanot{\overtwo{n_0}}{\overtwo{n_0 \sigsq_0}}.
\eeqn

i.e. the general normal-inverse-gamma. What is its kernel? Collect common terms and be neat.}\spc{9}


\hardsubproblem{[MA] If $\Xoneton~|~\theta, \sigsq \iid \normnot{\theta}{\sigsq}$ and given the general prior above, find the posterior and demonstrate it that the normal-inverse gamma is conjugate for the normal likelihood with both mean and variance unknown. This is what I did \emph{not} do in class but did last year.}\spc{13}

\end{enumerate}


\problem{We model the returns of S\&P 500 here.}

\begin{enumerate}
\easysubproblem{Below are the 16,428 daily returns (as a percentage) of the S\&P 500 dating back to January 4, 1950 and the code used to generate it. Does the data look normal? Yes/no}\spc{0}

\begin{figure}[h]
\centering
\includegraphics[width=7in]{daily_returns}
\end{figure}

\begin{verbatim}
X = read.csv('sp_tot_ret_price_1950.csv')
n = nrow(X)
n
hist(X[,4], br = 1000, 
  main = 'daily returns (as a percentage) of the S&P 500')
\end{verbatim}

\intermediatesubproblem{Do you think the data is $\iid$?}\spc{4}

\intermediatesubproblem{Assume $\iid$ normal data regardless of what you wrote in (a) and (b). The sample average is $\xbar=$0.0003415 and the sample standard deviation is $s=$0.0096. Under a relatively objective prior, give a 95\% credible region for the true mean daily return.}\spc{4}

\hardsubproblem{Give a 95\% credible region for \emph{tomorrow's} return.}\spc{4}

\end{enumerate}

\problem{This problem is about the normal-normal model using a \qu{semi-conjugate} prior. Assume $\Xoneton~|~\theta, \sigsq \iid \normnot{\theta}{\sigsq}$ throughout.}

\begin{enumerate}

\easysubproblem{If $\theta$ and $\sigsq$ are assumed to be independent, how can $\prob{\theta,~\sigsq}$ be factored?}\spc{1}

\easysubproblem{If $\theta \sim \normnot{\mu_0}{\tausq}$ and $\sigsq \sim \invgammanot{\overtwo{n_0}}{\overtwo{n_0 \sigsq_0}}$, find the kernel of $\prob{\theta,~\sigsq}$.}\spc{2}

\easysubproblem{Using your answer to (b), find the kernel of $\cprob{\theta,~\sigsq}{X}$}.\spc{2}

\hardsubproblem{Show that the kernel in (c) cannot be factored into the kernel of a normal and the kernel of an inverse gamma. This is in the lecture.}\spc{9}

\hardsubproblem{Your answer to (d) looks like a normal and a $k(\sigsq~|~X)$. Find the posterior mode of $\theta$.}\spc{10}

\hardsubproblem{Describe how you would sample from $\cprob{\theta,~\sigsq}{X}$. Make all steps explicit and use the notation from Table~\ref{tab:eqs}.}\spc{10}


\hardsubproblem{What are the disadvantages of grid sampling?}\spc{8}

\hardsubproblem{What's the bad part about not using a conjugate model?}\spc{4}


\extracreditsubproblem{[MA] Find the MMSE of $\sigsq$.}\spc{20}

\end{enumerate}




\end{document}

\problem This question is about building models for the prices of cars.

\begin{figure}[htp]
\centering
\includegraphics[width=2.7in]{accord.jpg}
\end{figure}

The 2016 Honda Accord sells at many different dealerships in New York City but sell it for more and some for less. We'll assume that the final negotiated price is distributed normally because it's most likely the sum of many different negotiation factors.

Our goal here is to determine the mean price at a certain car dealership in Astoria that people have been saying is \qu{too cheap} and if it's too cheap, Honda corporate may wish to investigate.


\begin{enumerate}

\easysubproblem{Assume that each Accord's price at the Astoria dealership is normal and $\iid$ given the parameters. Is this a good model? Why or why not? There is no \qu{correct} answer here but I expect you to defend whatever answer you write using the concepts we discussed in class.} \spc{5}

\easysubproblem{Despite what you wrote in (b), assume $\iid$ for the rest of the problem. The nationwide variance for a Honda Accord selling price we're going to assume is $\sigsq = \$1000^2$, an assumption we will relax later. Given a sample with average $\xbar$ and sample size $n$, what is the distribution of the mean price of a car from this shady Astoria dealership? Assume an uninformative prior of your choice but ensure to explicitly state it.}\spc{5}

\easysubproblem{You and your colleague go down to the Astoria dealership undercover and ask to buy a Honda. After much negotiation, they will sell it to you for \$19,000 and they will sell it to your colleague for \$18,200 but they sense something suspicious so you hesitate to send another one of your guys down there to do another faux negotiation. Unfortunately, we're going to have to estimate the mean with just $x_1=19000$ and $x_2 = 18200$. What is your best guess of the mean price of Honda Accords sold here? Assume your prior from (a). \compexpl }\spc{2}

\easysubproblem{What is the shrinkage value (which we have been denoting $\rho$) for this estimate? \compexpl}\spc{8}

\easysubproblem{Based on this data, we wish to test if this dealership is selling Honda Accords below the manufacturer sugested retail price (MSRP) of \$22,205 --- if so, they would be subject to a fine. Calculate a $p$-value for this test below by using notation from Table~\ref{tab:eqs} but do not solve numerically.}\spc{6}

\easysubproblem{What is the probability I get a really good deal --- that I can buy a car from these Astoria people for under \$17,000? Use the notation from Table~\ref{tab:eqs} but do not solve numerically.}\spc{3}


%\easysubproblem{If you were to estimate (g) without knowledge that $\sigma = \$1000$ but instead use \textit{an} uninformative prior (not necessarily \qu{the} uniformative prior) for $\sigsq$, would the probability of getting the same really good deal be greater than, less than or equal to your answer in (g)? Explain why.} \spc{8}

\easysubproblem{We will continue to not rely on the nationwide average of $\sigma = \$1000$. Here, instead of an uninformative prior, we use the Empirical Bayes concept to construct an informative prior (not uninformative).

Below are the sample average selling prices (in USD) of Honda Accords from 16 other car dealerships also in the NYC area that serve as a comparison: \\

\noindent
22889.80~ 21159.16 ~23796.71 ~19132.65 ~23450.63 ~24088.28 ~19852.37 ~21306.45 ~24434.05 ~23150.34 ~21690.09 ~20640.79 ~21973.45 ~21984.48 ~22326.00 ~22239.98\\


Using this data \textit{estimate} a conjugate prior for $\sigsq$. Use the $n_0$ and $\mu_0$ parameterization. You will still need $\sigsq$ from above!}\spc{4}

%\easysubproblem{We want to use the answer from (i) to fit a \textit{conjugate} normal-normal model (with $\sigsq$ unknown). This requires solving for $m$ in the $\cprob{\theta}{\sigsq}$ prior. So we set $s^2$ from (a) equal to $\sigsq / m$ and solve for $m$ and we get $m$ = 0.45 rounded to the nearest two digits. What is our prior on $\theta, \sigsq$ now? You can notate your answer in terms of standard densities and you do not have to simplify it to a kernel.} \spc{4}


\easysubproblem{Given the data in (d) which is $x_1=19000$ and $x_2 = 18200$, what is your best guess of the mean price of Honda Accords sold here? Assume the empirical Bayes conjugate prior. Round to the nearest cent.} \spc{2}


%\easysubproblem{If you were to answer (k) but this time assume an independent prior for $\theta$ from (a) and independent prior for $\sigsq$ from (i) and \textit{not} use the conjugate prior in (k), you would not be able to simply compute an estimate of mean price. Explain one way in which you could go about estimating this mean now. Provide one sentence of explanation \textit{only}. I am not looking for you to do any computation or describe a computer program.} \spc{6}


\end{enumerate}


\problem{We now continue questions on the normal-normal conjugate model.}

\begin{enumerate}

\hardsubproblem{[MA] Show that predictive distribution of $X^*~|~X,~\sigsq$ is normal if $\theta \sim \normnot{\mu_0}{\sigsq / n_0}$ by solving the integral and not using the convolution.}\spc{15}


\easysubproblem{If $\Xoneton~|~\theta,~\sigsq \iid  \normnot{\theta}{\sigsq}$ and $X$ represents all $\Xoneton$, in HW6 6(b) you found the kernel of $\sigsq~|~X,~\theta$. Show that this is the kernel of an inverse gamma. Use the $\sigsqhat$ substituion we did in class.}\spc{6}


\intermediatesubproblem{Why is using $\sigsqhat$ permitted in the setup in (a) but doesn't make sense in the ususal frequentist setup when the likelihood is normal? Hint: what is your target of estimation usually?}\spc{4}

\easysubproblem{In class we looked at $\sigsq \sim \invgammanot{\alpha}{\beta}$ but we used a different parameterization. Write the different parameterization below and explain why this was done i.e. interpret the meaning of the two new parameters.}\spc{6}

\intermediatesubproblem{Show that $\sigsq~|~X,~\theta$ is distributed as an inverse gamma with the prior from (d) and find its parameters.}\spc{9}

\easysubproblem{What is the Jeffrey's prior for $\sigsq$ (look in the notes and write it down --- no need to prove it). Is it proper?}\spc{2}

\easysubproblem{Show that the Jeffrey's prior for $\sigsq$ is an improper inverse gamma distribution and find its parameters. Note these parameters are not in the parameter space of a proper inverse gamma distribution.}\spc{2}

\easysubproblem{Under the Jeffrey's prior for $\sigsq$, what is the posterior?}\spc{2}


\intermediatesubproblem{You are in a milk manufacturing plant producing 1 quart cartons of whole milk. You are willing to assume that the nozzle emits 1 qt on average. In your previous job, you remember inspecting 3 cartons of which you saw 1.02, 0.97, 1.03 quarts of milk inside. Create a prior based on what you've seen in your previous job. This forces you to understand (d).}\spc{4}

\hardsubproblem{The company wishes to test if there's too much variability i.e. that there is more than $\sigma = 0.1$ variability. You take a sample of 10 and see 1.153, 1.045, 1.268, 1.333, 0.799, 1.075, 1.27, 1.07, 1.192 and 1.079 quarts. Find the $p$ value. You can write the answer below as a function of \texttt{rinvgamma}, \texttt{qinvgamma} or \texttt{pinvgamma} (i.e., expressions from Table~\ref{tab:eqs}). E.C. for computing it and testing this at $\alpha = 5\%$. You may want to use the \texttt{actuar} package (see \href{http://www.inside-r.org/packages/cran/actuar/docs/pinvgamma}{here}).}\spc{8}

\intermediatesubproblem{Find $CR_{\sigsq, 90\%}$ for the data above using expressions from Table~\ref{tab:eqs}. }\spc{12}

\end{enumerate}

\end{document}

\problem{These are questions about McGrayne's book, chapters 8-10.}

\begin{enumerate}

\easysubproblem{When was experimentation introduced to medical science and who introduced it? Are you surprised that it was this recent?}\spc{1}

\easysubproblem{Sir Ronald A. Fisher, the founder of modern experiments, did not believe cigarettes caused lung cancer. What were his two hypotheses for the cause of lung cancer?}\spc{2}

\easysubproblem{Who invented, and what are Bayes Factors? (p116)}\spc{2}

\easysubproblem{Trick question: who convinced Cornfield to stop smoking?}\spc{2}

\easysubproblem{Why were frequentists at a loss to estimate the probability of a nuclear bomb being detonated by accident?}\spc{2}

\easysubproblem{What is \href{https://en.wikipedia.org/wiki/Cromwell\%27s_rule}{Cromwell's Rule}? And, when applying this principle to a Bayesian model what would it imply? (See the Wikipedia link and p123).}\spc{2}

\easysubproblem{Did Bayesian Statistics prevent nuclear accidents? Discuss.}\spc{5}

\easysubproblem{What is the main reason why there are so many variations of Bayesian interpretation? (p129)}\spc{4}

\easysubproblem{What is a large practical drawback of Bayesian inference? (See mid-end of chapter 8).}\spc{9}

\end{enumerate}

\input{R_functions_table}

\problem{We will again be looking at the beta-prior, negative-binomial-likelihood Bayesian model. But first consider the more basic case where $\Xoneton ~|~ \theta \iid \geometric{\theta}$ and $\theta \sim \stdbetanot$.}


\begin{enumerate}


\easysubproblem{What is the likelihood model here? Write the PMF using the parameterization from class (not from previous classes you may have taken).}\spc{2}

\intermediatesubproblem{Demonstrate that the posterior in this case is Beta and find the posterior parameters.}\spc{6}

\easysubproblem{Give expressions for $\thetahatmmse,~\thetahatmae$ and $\thetahatmap$. Are they all similar if $n$ is large? Feel free to use Table~\ref{tab:eqs}.}\spc{3}

\hardsubproblem{Interpret the hyperparameters $\alpha$ and $\beta$ in the context of the posterior parameters.}\spc{5}



\easysubproblem{State the Jeffrey's prior for this model and explain why it is not proper.}\spc{2}

\easysubproblem{In what circumstances does Jeffrey's prior lead to a proper posterior?}\spc{2}

\easysubproblem{Given the posterior in (b), find the posterior in the case where you only observe one $x$.}\spc{2}

\hardsubproblem{You've seen the following data from the model: 5, 8, 6, 9, 11, 10, 7, 10, 11, 8, 9, 7, 7, 6. Design a prior using Empirical Bayes for $\theta$.}\spc{5}


\hardsubproblem{[MA] Use an objective prior. Imagine you now have seen 5 Bernoulli experiments go by with no success which you are patiently waiting for (since you can't use the cookie-cutter formulas until you see the success). Write an expression which if evaluated will provide the best guess of $\theta$ (best in a squared error loss sense) only given this \qu{partial} information. If you numerically compute it, you should get approximately 0.0238.}\spc{6}

%xs = seq(0, 100000)
%sum = 0
%alpha = 1
%beta = 1
%
%for (x in xs){
%	sum = sum + (alpha + 1)/(alpha+beta+1 + x) * beta(alpha + 1, x + beta) / beta(alpha, beta) 
%}
%sum


\intermediatesubproblem{[MA] Consider $\Xoneton ~|~\theta \iid \negbin{r}{\theta}$ where $r$ is considered known and $\theta \sim \stdbetanot$ Demonstrate that the posterior in this case is Beta and find the posterior parameters.}\spc{6}

\easysubproblem{[MA] Give expressions for $\thetahatmmse,~\thetahatmae$ and $\thetahatmap$ (use the approximation for the median of the beta distribution given in the notes for $\thetahatmae$). Are they all similar is $n$ is large?}\spc{3}


\easysubproblem{[MA] Find the Jeffrey's prior for $\theta$ as a function of $r$. Look up the $I(\theta)$ on the Internet for the negative binomial given the parameterization we used in class. You do not need to do the derivation yourself.}\spc{2}

\extracreditsubproblem{[MA] Derive the posterior prediction distribution PMF for one new negative binomial observation after seeing $n$ observations. This is a lot of computation.}\spc{18}

\extracreditsubproblem{[MA] Write an integral expression for the joint posterior distribution for $m$ new negative binomail observations. If you can find the solution somewhere on the Internet, no problemo.}\spc{6}

\intermediatesubproblem{Derive the PMF of the BetaGeometric($\alpha, \beta$) distribution. All you need to do is solve for the special case when $r=1$. Leave in terms of the beta function.}\spc{5}

\intermediatesubproblem{Find the kernel of the BetaGeometric distribution.}\spc{6}


\intermediatesubproblem{Imagine you've seen the following data from the $\iid$ geometric model: 5, 8, 6, 9, 11, 10, 7, 10, 11, 8, 9, 7, 7, 6. Use the Jeffrey's prior. Find a 80\% credible region for $\theta$.}\spc{3}

\easysubproblem{Find an expression for the probability the next observation will be 10.}\spc{2}

\intermediatesubproblem{Write an \texttt{R} expression for the probability in (r) using Table~\ref{tab:eqs}.}\spc{2}

\easysubproblem{[MA] Actually compute the probability in (r).}\spc{2}


\end{enumerate}



\problem{We will ask some basic problems on the Gamma-Poisson conjugate model.}

\begin{enumerate}

\easysubproblem{If $X \sim \poisson{\theta}$, what is the kernel of the posterior? Leave the prior in as a general term $\prob{\theta}$.}\spc{4}

\easysubproblem{Write the PDF of $\theta$ which is the gamma distribution with the standard hyperparameters we used in class.}\spc{2}

\easysubproblem{What is the support and parameter space?}\spc{2}

\easysubproblem{What is the expectation and standard error and mode?}\spc{2}


\easysubproblem{Draw four different pictures of different hyperparameter combinations to demonstrate this model's flexibility}\spc{10}


\intermediatesubproblem{Prove that the Poisson likelihood for $n=1$ with a gamma prior yields a gamma posterior and find its parameters.}\spc{4}

\intermediatesubproblem{Prove that the Poisson likelihood for $n$ observations with a gamma prior yields a gamma posterior and find its parameters.}\spc{8}

\intermediatesubproblem{For the Poisson likelihood for $n$ observations with a gamma prior find $\thetahatmmse$, $\thetahatmae$ and $\thetahatmap$.}\spc{2}

\intermediatesubproblem{[MA] Demonstrate that $\thetahatmmse$ is a shrinkage estimator and find $\rho$.}\spc{4}

\intermediatesubproblem{Demonstrate that $\prob{\theta} \propto 1$ is improper.}\spc{2}

\easysubproblem{[MA] Demonstrate that $\prob{\theta} \propto 1$ can be created by using an improper Gamma distribution (i.e. a Gamma distribution with parameters that are not technically in its parameter space and thereby does not admit a distribution function).}\spc{2}

\easysubproblem{What is the Jeffrey's prior for the Poisson likelihood model? Do not rederive. Just copy.}\spc{2}

\easysubproblem{What is the equivalent of the Haldane prior in the Binomial likelihood model for the Poisson likelihood model?}\spc{2}

\extracreditsubproblem{[MA] Prove that posterior predictive distribution for the next Poisson realization given $n$ observed Poisson realizations is negative binomially distributed and show its parameters are $p = \beta / (\beta + 1)$ and $r = \alpha$ for $\alpha \in \naturals$.}\spc{10}

\intermediatesubproblem{If $\alpha \notin \naturals$, create an \qu{extended negative binomial} r.v. and find its PMF. You can copy from Wikipedia.}\spc{3}


\intermediatesubproblem{Why is the extended negative binomial r.v. also known as the gamma-Poisson mixture distribution?}\spc{5}

\extracreditsubproblem{[MA] Find the joint posterior predictive distribution for $m$ future observations. I couldn't find the answer to this myself nor compute the integral.}\spc{5}

\intermediatesubproblem{If you observe $0,3,2,4,2,6,1,0,5$, give a 95\% CR for $\theta$. Pick an objective prior.}\spc{5}

\intermediatesubproblem{Using the data and the prior from (r), test if $\theta < 2$.}\spc{8}


\extracreditsubproblem{[MA] We talked about that the negative binomial is an \qu{overdispersed} Poisson. Show that the negative binomial converges to a Poisson. Try yourself before you Google the answer.}\spc{5}

\end{enumerate}


\problem{We will ask some basic problems on the Gamma-Exponential conjugate model.}

\begin{enumerate}

\easysubproblem{If $X \sim \exponential{\theta}$, what is the kernel of $\theta~|~X$?}\spc{5}

\intermediatesubproblem{Prove that the Exponential likelihood for $n=1$ with a gamma prior yields a gamma posterior and find its parameters.}\spc{8}

\intermediatesubproblem{Prove that the exponential likelihood for $n$ observations with a gamma prior yields a gamma posterior and find its parameters.}\spc{6}

\intermediatesubproblem{For the exponential likelihood for $n$ observations with a gamma prior find $\thetahatmmse$, $\thetahatmae$ and $\thetahatmap$.}\spc{4}

\intermediatesubproblem{Demonstrate that $\thetahatmmse$ is a shrinkage estimator and find $\rho$.}\spc{4}

\intermediatesubproblem{Use an uninformative prior like in the previous question. What is the posterior?}\spc{5}

\intermediatesubproblem{Write the integral to solve for the posterior predictive distribution for a single observation given $n$ observed data points.}\spc{5}

\hardsubproblem{[MA] Solve the integral.}\spc{6}

\extracreditsubproblem{[MA] Find the joint posterior predictive distribution for $m$ future observations.}\spc{0.1}

\extracreditsubproblem{[MA] What is the Jeffrey's prior for the exponential likelihood? Try yourself before you Google the answer.}\spc{5}

\end{enumerate}


\problem{We now begin the normal-normal conjugate model.}

\begin{enumerate}

\easysubproblem{If $X \sim \normnot{\theta}{\sigsq}$, what is the kernel of $\theta~|~X,~\sigsq$?}\spc{1}

\easysubproblem{If $X \sim \normnot{\theta}{\sigsq}$, what is the kernel of $\sigsq~|~X,~\theta$?}\spc{1}

\easysubproblem{If $X \sim \normnot{\theta}{\sigsq}$, what is the kernel of $\theta~,\sigsq~|~X$?}\spc{1}

\hardsubproblem{Show that posterior of $\theta~|~X,~\sigsq$ is normal if $\theta \sim \normnot{\mu_0}{\tausq}$. Try to do it yourself and only copy from the notes if you have to.}\spc{15}

\end{enumerate}


\end{document}

\easysubproblem{What is the definition of the convolution of two r.v.'s $X_1$ and $X_2$?}\spc{1}

\hardsubproblem{Show that predictive distribution of $X^*~|~X,~\sigsq$ is normal if $\theta \sim \normnot{\mu_0}{\tausq}$ by solving the integral and not using the convolution.}\spc{12}

\hardsubproblem{Even though you solved this in (f), using the law of iterated expectation, find the expectation of the predictive distribution of $X^*~|~X,~\sigsq$.}\spc{8}

\hardsubproblem{Even though you solved this in (f), Using the law of total variance, find the variance of the predictive distribution of $X^*~|~X,~\sigsq$.}\spc{6}


\easysubproblem{In this problem we found the posterior, $\theta~|~X,~\sigsq$. What are all the other posteriors that could be of interest? Explain the inferential targets of each.}\spc{6}

\end{enumerate}


\end{document}
